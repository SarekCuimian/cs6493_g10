{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Refine Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting dashscope\n",
      "  Using cached dashscope-1.22.2-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\programdata\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: urllib3 in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from dashscope) (3.10.11)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from dashscope) (2.32.3)\n",
      "Requirement already satisfied: websocket-client in c:\\programdata\\anaconda3\\lib\\site-packages (from dashscope) (1.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->dashscope) (5.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->dashscope) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->dashscope) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->dashscope) (2025.1.31)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from multidict<7.0,>=4.5->aiohttp->dashscope) (4.11.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp->dashscope) (0.2.0)\n",
      "Using cached dashscope-1.22.2-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: dashscope\n",
      "Successfully installed dashscope-1.22.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script dashscope.exe is installed in 'C:\\Users\\ab-in\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install dashscope python-dotenv urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dashscope\n",
    "import sys\n",
    "import platform\n",
    "import dashscope\n",
    "import time\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "from typing import Tuple, Optional, Dict, Union, List, Any\n",
    "from dashscope.api_entities.dashscope_response import Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current environment information\n",
      "----------------------------------------\n",
      "Python version: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]\n",
      "Platform system: Windows 10\n",
      "DASHSCOPE_API_KEY read successfully: ✅ Yes\n"
     ]
    }
   ],
   "source": [
    "# 1) Load environment (DASHSCOPE_API_KEY)\n",
    "\n",
    "load_dotenv(\"dashscope_api_key.env\", override=True)\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"DASHSCOPE_API_KEY not found!\")\n",
    "\n",
    "# Print environment information\n",
    "print(\"Current environment information\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform system: {platform.system()} {platform.release()}\")\n",
    "print(f\"DASHSCOPE_API_KEY read successfully: {'✅ Yes' if api_key else '❌ No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Call Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **`call_model`** function is designed to invoke a large language model (LLM) through DashScope in a way that **preserves multi-round context**. Rather than passing a one-time prompt, it accepts a **list of messages**—each labeled with a role (`system`, `user`, or `assistant`)—that collectively represent the conversation history. This approach enables an iterative, dialogic style of interaction with the model:\n",
    "\n",
    "1. **Conversation as a List of Messages**  \n",
    "   - We store every piece of text that shapes the conversation (system instructions, user questions, model replies) in a Python list called `messages`.  \n",
    "   - Each element is a structure with fields like `role` (e.g., `system`, `user`, `assistant`) and `content` (the actual text).\n",
    "\n",
    "2. **System-Level Instructions**  \n",
    "   - At the start of the list, we often include a “system” message that defines the model’s role or behavior (“You are a rigorous math tutor…”).  \n",
    "   - Unlike a single-pass prompt, this system message persists throughout every exchange, ensuring the LLM remains consistent in style and purpose.\n",
    "\n",
    "3. **User Queries and Model Responses**  \n",
    "   - For each round of interaction, we append a new `user` message with the user’s request or instructions.  \n",
    "   - The model’s reply then comes back as `assistant` text, which we likewise insert into the `messages` list.  \n",
    "   - Over time, this list accumulates the entire conversation history.\n",
    "\n",
    "4. **Self-Refine Alignment**  \n",
    "   - Because all prior messages (including the model’s own previous answers) are visible in the list, the LLM can read and critique its own past output.  \n",
    "   - This structure naturally supports the **feedback → refinement** cycle described in the Self-Refine paradigm, as the model sees both the problem statement and its earlier reasoning steps whenever it generates new content.\n",
    "\n",
    "5. **DashScope Invocation**  \n",
    "   - On each call, `call_model` sends the current `messages` list to the DashScope service, specifying the temperature and other parameters.  \n",
    "   - DashScope’s API returns the model’s fresh reply (an “assistant” role string) and usage statistics (like token counts).  \n",
    "   - We update the `messages` with the new assistant message, thus preserving context for the next round.\n",
    "\n",
    "6. **Context Preservation**  \n",
    "   - By passing **the entire conversation history** each time, we mimic the idea of a session that retains prior knowledge, enabling multi-step problem-solving or iterative refinement.  \n",
    "   - This stands in contrast to single-prompt APIs that only see their immediate input. Here, the model is re-supplied with both the user’s original questions and any subsequent instructions or feedback, capturing the logic of self-critique and incremental improvement.\n",
    "\n",
    "7. **Benefits and Considerations**  \n",
    "   - **Enhanced Consistency**: The model maintains continuity across multiple rounds, remembering earlier details or constraints.  \n",
    "   - **Scalability**: Because the conversation grows each iteration, users should watch out for token limit constraints and trim older or less relevant messages if needed.  \n",
    "   - **Clearer Control Flow**: The code that orchestrates Self-Refine can systematically add new messages (feedback requests, draft solutions, refined outputs, etc.), making the dialogue flow explicit and easy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(\n",
    "    messages: List[Message],\n",
    "    model_name: str = \"qwen2.5-math-1.5b-instruct\",\n",
    "    temperature: float = 0.7,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: float = 1.0,\n",
    "    verbose: bool = False\n",
    ") -> Tuple[Optional[str], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Call the DashScope model with a list of messages (including system, user, assistant).\n",
    "\n",
    "    Returns:\n",
    "        content: The assistant's text response (None if error).\n",
    "        usage:   usage dict (may include token counts, etc.), or {}\n",
    "    \"\"\"\n",
    "\n",
    "    api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"DASHSCOPE_API_KEY not found!\")\n",
    "        return None, {}\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = dashscope.Generation.call(\n",
    "                api_key=api_key,\n",
    "                model=model_name,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                result_format=\"message\",\n",
    "                timeout=60\n",
    "            )\n",
    "\n",
    "            status_code = response.get(\"status_code\", 200)  # type: ignore\n",
    "            if status_code == 429:\n",
    "                if verbose:\n",
    "                    print(f\"⚠️[Attempt {attempt}/{max_retries}] Rate limit! Retrying in {retry_delay}s...\")\n",
    "                time.sleep(retry_delay)\n",
    "                continue\n",
    "\n",
    "            # Parse out the model's reply\n",
    "            content = response[\"output\"][\"choices\"][0][\"message\"][\"content\"]  # type: ignore\n",
    "            usage = response.get(\"usage\", {})  # type: ignore\n",
    "            return content, usage\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️[Attempt {attempt}/{max_retries}] Exception occurred: {e}\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    print(f\"❌ All {max_retries} attempts failed.\")\n",
    "    return None, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model Output ===\n",
      "Assistant Response:\n",
      " To determine how many cupcakes the baker has left, we need to follow these steps:\n",
      "\n",
      "1. Start with the initial number of cupcakes the baker has, which is 20.\n",
      "2. Subtract the number of cupcakes the baker eats, which is 2.\n",
      "3. Subtract the number of cupcakes the baker gives away, which is 3.\n",
      "\n",
      "Let's perform the calculations step-by-step:\n",
      "\n",
      "1. Initial number of cupcakes: 20\n",
      "2. After eating 2 cupcakes: \\(20 - 2 = 18\\)\n",
      "3. After giving away 3 cupcakes: \\(18 - 3 = 15\\)\n",
      "\n",
      "So, the number of cupcakes the baker has left is \\(\\boxed{15}\\).\n",
      "Usage: {\"input_tokens\": 97, \"output_tokens\": 149, \"total_tokens\": 246, \"cached_tokens\": 0}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    question = \"A baker makes 20 cupcakes. He eats 2 and gives away 3. How many does he have left?\"\n",
    "    system_prompt = (\n",
    "        \"You are a rigorous math tutor. \"\n",
    "        \"You solve problems step by step in detail and ensure mathematical correctness. \"\n",
    "        \"Then, at the end, on a new line, write:\\n\"\n",
    "        \"Final Answer: <numeric result>\\n\"\n",
    "        \"Do not include any additional text after that line.\"\n",
    "    )\n",
    "    messages = [\n",
    "        Message(role=\"system\", content=system_prompt),\n",
    "        Message(role=\"user\", content=f\"Solve the following problem step-by-step:\\n{question}\")\n",
    "    ]\n",
    "\n",
    "    content, usage = call_model(\n",
    "        messages=messages,\n",
    "        model_name=\"qwen2.5-math-1.5b-instruct\",\n",
    "        temperature=0.2,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Model Output ===\")\n",
    "    print(\"Assistant Response:\\n\", content)\n",
    "    print(\"Usage:\", usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(response_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the final numeric or concise answer from the given response text.\n",
    "\n",
    "    The order of priority is as follows:\n",
    "    1. Check \"Final Answer: ...\" (case-insensitive).\n",
    "    2. Check the last occurrence of \"\\boxed{...}\" (LaTeX style). From that box, parse advanced numeric forms.\n",
    "    3. If no box found or no valid numeric in it, scan the entire text for the last recognized numeric form.\n",
    "    4. If nothing is found at all, return the entire trimmed response text.\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Check for \"Final Answer: ...\"\n",
    "    #    (Now has the highest priority)\n",
    "    # ----------------------------\n",
    "    fa_match = re.search(r\"(?i)final answer\\s*:\\s*([^\\n]+)\", response_text)\n",
    "    if fa_match:\n",
    "        candidate = fa_match.group(1).strip()\n",
    "        # If we want to parse advanced numeric forms from this candidate,\n",
    "        # we can do so here. Otherwise, we return directly.\n",
    "        return candidate\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Check for the last occurrence of \"\\boxed{...}\"\n",
    "    #    We'll parse advanced numeric forms from within it.\n",
    "    #    If multiple boxes exist, we take the last one.\n",
    "    # ----------------------------\n",
    "    # Regex to match all \\boxed{...} blocks (including nested braces is tricky, but we do a simple approach)\n",
    "    all_boxes = re.findall(r\"\\\\boxed\\{([^}]*)\\}\", response_text, flags=re.DOTALL)\n",
    "    if all_boxes:\n",
    "        box_content = all_boxes[-1]  # take the last box\n",
    "        # Try to parse advanced numeric forms from the box content\n",
    "        numbers_in_box = parse_numeric_expressions(box_content)\n",
    "        if numbers_in_box:\n",
    "            # If multiple numbers found, return the last one\n",
    "            return numbers_in_box[-1].strip()\n",
    "        # If no recognized numeric, return the raw content\n",
    "        return box_content.strip()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Fallback: parse the entire text for recognized numeric forms\n",
    "    #    Return the last one if available.\n",
    "    # ----------------------------\n",
    "    all_numbers = parse_numeric_expressions(response_text)\n",
    "    if all_numbers:\n",
    "        return all_numbers[-1].strip()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) Final fallback: return entire string\n",
    "    # ----------------------------\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "def parse_numeric_expressions(text: str):\n",
    "    \"\"\"\n",
    "    Parse various numeric expressions from the text, including:\n",
    "    - Optional sign: [+/-]\n",
    "    - Integers or decimals (e.g., 123, -3.14)\n",
    "    - Simple LaTeX fraction: \\frac{number}{number}\n",
    "\n",
    "    Returns a list of all matched string forms in order of appearance.\n",
    "    \"\"\"\n",
    "    # We'll combine a few patterns:\n",
    "    # 1) signed integer or decimal: [+-]?\\d+(?:\\.\\d+)?\n",
    "    # 2) simple fraction form: \\frac\\s*\\{?\\s*[+-]?\\d+(\\.\\d+)?\\s*\\}?\\s*/\\s*\\{?\\s*[+-]?\\d+(\\.\\d+)?\\s*\\}?\n",
    "    pattern = r\"\"\"\n",
    "        [+-]?\\d+(?:\\.\\d+)?              # e.g. -3, 4.5, +2.0\n",
    "        |\n",
    "        \\\\frac\\s*\\{?\\s*[+-]?\\d+(?:\\.\\d+)?\\s*\\}?\\s*/\\s*\\{?\\s*[+-]?\\d+(?:\\.\\d+)?\\s*\\}?  # e.g. \\frac{3}{5}, \\frac{-2.5}{10}\n",
    "    \"\"\"\n",
    "\n",
    "    # Using VERBOSE flag to allow inline comments\n",
    "    matches = re.findall(pattern, text, flags=re.VERBOSE)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `self_refine` function implements a multi-round refinement process for math problem solving, following the Self-Refine paradigm. It uses a single, persistent `messages` list to simulate a continuous dialogue with the model, allowing it to build upon its own previous answers and feedback.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Initial Draft**\n",
    "   - The function begins by prompting the model to generate a step-by-step solution to the math problem.\n",
    "   - This initial response is saved as an assistant message in the session.\n",
    "\n",
    "2. **Feedback and Refinement Loop**\n",
    "   - Each round, the model is asked to critique its last answer.\n",
    "   - It then receives its own feedback and is prompted to revise accordingly.\n",
    "   - All exchanges (questions, answers, feedback) are appended to the same `messages` list, preserving full context.\n",
    "\n",
    "3. **Session-Based Memory**\n",
    "   - By keeping the entire conversation in a single session, the model sees the full reasoning history at every step.\n",
    "   - This allows coherent self-correction and aligns with the original Self-Refine paper’s methodology.\n",
    "\n",
    "4. **Early Stopping (with Safeguard)**\n",
    "   - The model may exit early if it declares the answer already correct — but only after completing at least one refinement cycle.\n",
    "   - This ensures robustness without redundant computation.\n",
    "\n",
    "5. **Final Output**\n",
    "   - The last refined answer is parsed to extract the final numeric result.\n",
    "   - The function returns this result, the full answer text, number of rounds used, and total token usage.\n",
    "\n",
    "### Key Points & Considerations\n",
    "\n",
    "- **Rigorous System Prompt**  \n",
    "  - The model is cast in the role of a “rigorous math tutor,” ensuring detailed reasoning and clarity in every generation.\n",
    "  - >Since both temperature and top_p can control the diversity of generated text, it is recommended that you only set one of them. \n",
    "    - So I lowered the temperature a bit, reduced the diversity appropriately, and ensured the rigor of the mathematical results.\n",
    "\n",
    "- **Feedback-Refine Loop**  \n",
    "  The model alternates between critiquing its own output and improving it, thereby aiming to catch logical or computational mistakes in a structured manner.\n",
    "\n",
    "- **Mandatory Improvement Round**  \n",
    "  The function will not accept an immediate “no improvement” response for the first refinement attempt. This ensures that, at a minimum, the model has made one pass to reevaluate its initial draft.\n",
    "\n",
    "- **Early Stop Heuristic**  \n",
    "  After at least one refinement, if the model states that no further changes are needed, the loop terminates early. This method is a convenience to reduce overhead when the model is sufficiently confident.\n",
    "\n",
    "- **No Absolute Guarantee of Correctness**  \n",
    "  While the approach often enhances solution quality, the model’s own judgment may still fail to catch subtler errors. For high-stakes scenarios, external validation (e.g., test suites or domain experts) is recommended.\n",
    "\n",
    "- **Return Structure**  \n",
    "  The final output includes:\n",
    "  1. **`final_text`**: The model’s last refined draft (full textual answer).  \n",
    "  2. **`final_answer`**: The numeric or short result extracted from `final_text`.  \n",
    "  3. **`ended_on_round`**: Which round triggered the exit, either because of reaching the maximum iteration or early stopping.  \n",
    "  4. **`total_usage`**: A cumulative usage metric (e.g., total tokens).\n",
    "\n",
    "This procedure illustrates a practical way to nudge large language models toward more self-critical and iteratively improved answers, particularly for math-oriented tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_refine(\n",
    "    question: str,\n",
    "    rounds: int = 3,\n",
    "    verbose: bool = True,\n",
    "    model_name: str = \"qwen2.5-math-1.5b-instruct\",\n",
    "    temperature: float = 0.2,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Conduct multiple rounds of 'Self-Refine' for a math problem in a single session (messages list),\n",
    "    returning the final refined answer.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The math problem to be solved.\n",
    "        rounds (int): The maximum number of refinement iterations.\n",
    "        verbose (bool): Whether to print intermediate steps.\n",
    "        model_name (str): The name of the language model to call.\n",
    "        temperature (float): Sampling temperature for controlling randomness in the output.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "          {\n",
    "              \"final_text\"      : str or None, \n",
    "              \"final_answer\"    : str or None,\n",
    "              \"ended_on_round\"  : int,\n",
    "              \"total_usage\"     : int  # or float, depending on actual usage measure\n",
    "          }\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # 1) Construct an initial 'messages' list\n",
    "    # -----------------------------\n",
    "    # (A) system prompt\n",
    "    system_content = (\n",
    "        \"You are a rigorous math tutor. \"\n",
    "        \"You solve problems step by step in detail and ensure mathematical correctness. \"\n",
    "        \"When you finish, on a new line, write:\\n\"\n",
    "        \"Final Answer: <numeric result>\\n\"\n",
    "        \"Do not include any additional text after that line.\"\n",
    "    )\n",
    "    messages = [Message(role=\"system\", content=system_content)]\n",
    "\n",
    "    # (B) user: problem statement\n",
    "    #   This is akin to the old \"first_prompt_template\"\n",
    "    user_intro = (\n",
    "        \"Solve the following math problem with detailed reasoning. \"\n",
    "        \"Explain your steps clearly and logically. \"\n",
    "        \"At the end, please write:\\n\"\n",
    "        \"Final Answer: <number>\\n\\n\"\n",
    "        f\"{question}\"\n",
    "    )\n",
    "    messages.append(Message(role=\"user\", content=user_intro))\n",
    "    \n",
    "    total_usage_tokens = 0\n",
    "    ended_on_round = 0\n",
    "\n",
    "    # -----------------------------\n",
    "    # 2) Generate the initial draft (assistant)\n",
    "    # -----------------------------\n",
    "    initial_draft, usage_init = call_model(\n",
    "        messages=messages,                # <--- We pass the entire messages so far\n",
    "        model_name=model_name,\n",
    "        temperature=temperature,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    if usage_init is None:\n",
    "        usage_init = {\"total_tokens\": 0}\n",
    "    total_usage_tokens += usage_init[\"total_tokens\"]\n",
    "\n",
    "    if initial_draft is None:\n",
    "        if verbose:\n",
    "            print(\"❌[Error] Initial draft generation returned None.\")\n",
    "        return {\n",
    "            \"final_text\"    : None,\n",
    "            \"final_answer\"  : None,\n",
    "            \"ended_on_round\": 0,\n",
    "            \"total_usage\"   : total_usage_tokens\n",
    "        }\n",
    "\n",
    "    # Save the assistant's response to messages\n",
    "    messages.append(Message(role=\"assistant\", content=initial_draft))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n✅━━ [Initial Draft Generated] ━━✅\")\n",
    "        print(initial_draft)\n",
    "        print(\"\")\n",
    "\n",
    "    current_answer = initial_draft\n",
    "\n",
    "    # -----------------------------\n",
    "    # 3) Iterative refinement\n",
    "    # -----------------------------\n",
    "    for i in range(rounds):\n",
    "        round_index = i + 1\n",
    "        if verbose:\n",
    "            print(f\"[Refinement Round {round_index} of {rounds}]\")\n",
    "\n",
    "        # 3a) user asks for feedback\n",
    "        feedback_request = (\n",
    "            f\"Below is the math problem and your draft solution. Please analyze it:\\n\\n\"\n",
    "            f\"Problem:\\n{question}\\n\\n\"\n",
    "            f\"Draft Answer:\\n{current_answer}\\n\\n\"\n",
    "            \"Point out any errors or omissions, suggest improvements. \"\n",
    "            \"If it's already correct, say 'no improvement needed'.\"\n",
    "        )\n",
    "        messages.append(Message(role=\"user\", content=feedback_request))\n",
    "\n",
    "        feedback, usage_fb = call_model(\n",
    "            messages=messages,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if usage_fb is None:\n",
    "            usage_fb = {\"total_tokens\": 0}\n",
    "        total_usage_tokens += usage_fb[\"total_tokens\"]\n",
    "\n",
    "        if feedback is None:\n",
    "            if verbose:\n",
    "                print(\"❌[Error] Feedback generation returned None. Stopping.\")\n",
    "            ended_on_round = round_index\n",
    "            break\n",
    "\n",
    "        # Save model's feedback to messages (assistant role)\n",
    "        messages.append(Message(role=\"assistant\", content=feedback))\n",
    "\n",
    "        if verbose:\n",
    "            print(\"✅[Feedback Received]\")\n",
    "            print(feedback)\n",
    "            print(\"\")\n",
    "\n",
    "        # 3b) check if \"no improvement\" (but only if round_index > 1 to ensure at least 1 round of refine)\n",
    "        lower_fb = feedback.lower()\n",
    "        if round_index > 1 and (\n",
    "            \"no improvement\" in lower_fb\n",
    "            or \"solution is correct\" in lower_fb\n",
    "            or \"already correct\" in lower_fb\n",
    "        ):\n",
    "            if verbose:\n",
    "                print(\"✅[Notice] Feedback indicates no further improvement is needed. Stopping early.\")\n",
    "            ended_on_round = round_index\n",
    "            break\n",
    "\n",
    "        # 3c) user instructs to refine\n",
    "        refine_request = (\n",
    "            f\"Here is the math problem, your draft solution, and the feedback:\\n\\n\"\n",
    "            f\"Problem:\\n{question}\\n\\n\"\n",
    "            f\"Draft Answer:\\n{current_answer}\\n\\n\"\n",
    "            f\"Feedback:\\n{feedback}\\n\\n\"\n",
    "            \"Please refine the draft solution. \"\n",
    "            \"At the end, write 'Final Answer: <number>'. \"\n",
    "            \"Make sure the numeric result is correct.\"\n",
    "        )\n",
    "        messages.append(Message(role=\"user\", content=refine_request))\n",
    "\n",
    "        refined_answer, usage_refine = call_model(\n",
    "            messages=messages,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if usage_refine is None:\n",
    "            usage_refine = {\"total_tokens\": 0}\n",
    "        total_usage_tokens += usage_refine[\"total_tokens\"]\n",
    "\n",
    "        if refined_answer is None:\n",
    "            if verbose:\n",
    "                print(\"⚠️[Warning] Refinement returned None. Using current version and stopping.\")\n",
    "            ended_on_round = round_index\n",
    "            break\n",
    "\n",
    "        # Save new refined answer as assistant\n",
    "        messages.append(Message(role=\"assistant\", content=refined_answer))\n",
    "        current_answer = refined_answer\n",
    "\n",
    "        if verbose:\n",
    "            print(\"✅[Refined Answer]\")\n",
    "            print(current_answer)\n",
    "            print(\"\")\n",
    "\n",
    "        ended_on_round = round_index\n",
    "\n",
    "    # -----------------------------\n",
    "    # 4) Extract final numeric answer\n",
    "    # -----------------------------\n",
    "    final_answer = extract_final_answer(current_answer)\n",
    "\n",
    "    # -----------------------------\n",
    "    # 5) Return\n",
    "    # -----------------------------\n",
    "    return {\n",
    "        \"final_text\"    : current_answer,\n",
    "        \"final_answer\"  : final_answer,\n",
    "        \"ended_on_round\": ended_on_round,\n",
    "        \"total_usage\"   : total_usage_tokens\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Single Demo Question Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅━━ [Initial Draft Generated] ━━✅\n",
      "To determine how much Janet makes every day at the farmers' market, we need to follow these steps:\n",
      "\n",
      "1. Calculate the total number of eggs laid by the ducks each day.\n",
      "2. Determine how many eggs Janet sells at the farmers' market each day.\n",
      "3. Calculate the revenue from selling the eggs at the farmers' market.\n",
      "\n",
      "**Step 1: Calculate the total number of eggs laid by the ducks each day.**\n",
      "\n",
      "Janet's ducks lay 16 eggs per day.\n",
      "\n",
      "**Step 2: Determine how many eggs Janet sells at the farmers' market each day.**\n",
      "\n",
      "Janet eats 3 eggs for breakfast every morning and bakes 4 eggs for her friends every day. Therefore, the total number of eggs she does not sell is:\n",
      "\\[ 3 + 4 = 7 \\text{ eggs} \\]\n",
      "\n",
      "The number of eggs Janet sells at the farmers' market each day is:\n",
      "\\[ 16 - 7 = 9 \\text{ eggs} \\]\n",
      "\n",
      "**Step 3: Calculate the revenue from selling the eggs at the farmers' market.**\n",
      "\n",
      "Janet sells each egg for $2. Therefore, the revenue from selling 9 eggs is:\n",
      "\\[ 9 \\times 2 = 18 \\text{ dollars} \\]\n",
      "\n",
      "So, the amount Janet makes every day at the farmers' market is:\n",
      "\\[\n",
      "\\boxed{18}\n",
      "\\]\n",
      "\n",
      "[Refinement Round 1 of 3]\n",
      "✅[Feedback Received]\n",
      "The solution provided is correct. There are no errors or omissions. The steps are clear and logical, and the final answer is accurate. Therefore, no improvement is needed. The final answer is:\n",
      "\\[\n",
      "\\boxed{18}\n",
      "\\]\n",
      "\n",
      "✅[Refined Answer]\n",
      "The solution provided is correct. There are no errors or omissions. The steps are clear and logical, and the final answer is accurate. Therefore, no improvement is needed. The final answer is:\n",
      "\\[\n",
      "\\boxed{18}\n",
      "\\]\n",
      "\n",
      "[Refinement Round 2 of 3]\n",
      "✅[Feedback Received]\n",
      "The solution provided is correct. There are no errors or omissions. The steps are clear and logical, and the final answer is accurate. Therefore, no improvement is needed. The final answer is:\n",
      "\\[\n",
      "\\boxed{18}\n",
      "\\]\n",
      "\n",
      "✅[Notice] Feedback indicates no further improvement is needed. Stopping early.\n",
      "\n",
      "━━━ ★  SINGLE QUESTION TEST  ★ ━━━\n",
      "📌  Question:\n",
      "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "\n",
      "💡  Predicted Final Answer: 18\n",
      "🎯  Gold Answer: 18\n",
      "✅  Correct? True\n",
      "🔎  Response Length (chars): 209\n",
      "🌀  Ended on Round: 2\n",
      "📊  Total Usage (tokens): 4333\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "\n",
    "    # Gold answer\n",
    "    gold_answer = \"18\"\n",
    "\n",
    "    # Do self-consistency\n",
    "    result = self_refine(\n",
    "        question=question,\n",
    "        model_name=\"qwen2.5-math-1.5b-instruct\",\n",
    "        rounds=3\n",
    "    )\n",
    "    final_text = result[\"final_text\"]\n",
    "    final_answer = result[\"final_answer\"]\n",
    "    ended_on_round = result[\"ended_on_round\"]\n",
    "    total_usage = result[\"total_usage\"]\n",
    "\n",
    "    # Compare with gold\n",
    "    is_correct = (final_answer.strip() == gold_answer.strip())\n",
    "    response_length = len(final_text)\n",
    "\n",
    "    \n",
    "    # Print the info\n",
    "    print(\"\\n━━━ ★  SINGLE QUESTION TEST  ★ ━━━\")\n",
    "    print(f\"📌  Question:\\n{question}\\n\")\n",
    "    print(f\"💡  Predicted Final Answer: {final_answer}\")\n",
    "    print(f\"🎯  Gold Answer: {gold_answer}\")\n",
    "    print(f\"✅  Correct? {is_correct}\")\n",
    "    print(f\"🔎  Response Length (chars): {response_length}\")\n",
    "    print(f\"🌀  Ended on Round: {ended_on_round}\")\n",
    "    print(f\"📊  Total Usage (tokens): {total_usage}\")\n",
    "    print(\"━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
