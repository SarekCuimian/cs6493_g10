{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Consistency Test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:13:08.814256Z",
     "start_time": "2025-03-30T14:13:08.684018Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import dashscope\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "###############################################################################\n",
    "# 1) Load environment (DASHSCOPE_API_KEY)\n",
    "###############################################################################\n",
    "\n",
    "load_dotenv(\"dashscope_api_key.env\")\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"‚ùå DASHSCOPE_API_KEY not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define call_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:13:08.819990Z",
     "start_time": "2025-03-30T14:13:08.817244Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 2) Single call to Qwen via DashScope\n",
    "###############################################################################\n",
    "def call_model(\n",
    "    prompt: str,\n",
    "    model_name: str = \"deepseek-r1-distill-qwen-1.5b\",  # use a valid DashScope model name\n",
    "    system_prompt: str = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Please show your reasoning step by step (Chain of Thought). \"\n",
    "        \"Then, on a new line at the end, write:\\n\"\n",
    "        \"'Final Answer: <the numeric result>'\"\n",
    "    ),\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Call Qwen via DashScope with minimal error handling and an explicit prompt\n",
    "    that instructs the model to produce a final line 'Final Answer: ...'.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = dashscope.Generation.call(\n",
    "            api_key=api_key,\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            result_format=\"message\"\n",
    "        )\n",
    "\n",
    "        status_code = response.get(\"status_code\", None)\n",
    "        if status_code == 429:\n",
    "            print(f\"‚è≥ [Attempt {attempt+1}/{max_retries}] Rate limit! Wait {retry_delay}s...\")\n",
    "            time.sleep(retry_delay)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Extract the content and token usage from the response\n",
    "            content = response[\"output\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            usage = response.get(\"usage\", {})\n",
    "            return content, usage \n",
    "        \n",
    "        except (TypeError, KeyError, IndexError):\n",
    "            print(f\"‚ö†Ô∏è [Attempt {attempt+1}/{max_retries}] Unexpected structure:\\n{response}\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    print(f\"‚ùå All {max_retries} attempts failed, returning empty.\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract answer from model response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:13:13.841273Z",
     "start_time": "2025-03-30T14:13:13.838587Z"
    }
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# 3) Extract \"Final Answer: x\"\n",
    "###############################################################################\n",
    "\n",
    "def extract_final_answer(response_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract the final numeric answer from the model response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1) Try \\boxed{...} with possible nested braces\n",
    "    box_match = re.search(r\"\\\\boxed\\{([^}]*)\\}\", response_text, flags=re.DOTALL)\n",
    "    if box_match:\n",
    "        box_content = box_match.group(1)\n",
    "        # Grab the first number from within the box\n",
    "        numbers_in_box = re.findall(r\"\\d+(?:\\.\\d+)?\", box_content)\n",
    "        if numbers_in_box:\n",
    "            return numbers_in_box[0].strip()\n",
    "        # If no number in the box, just return the raw content\n",
    "        return box_content.strip()\n",
    "\n",
    "    # 2) Try \"Final Answer: ...\"\n",
    "    fa_match = re.search(r\"(?i)final answer\\s*:\\s*([^\\n]*)\", response_text)\n",
    "    if fa_match:\n",
    "        return fa_match.group(1).strip()\n",
    "\n",
    "    # 3) Fallback: last number in the text\n",
    "    numbers = re.findall(r\"\\d+(?:\\.\\d+)?\", response_text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "\n",
    "    # 4) Final fallback: return entire string\n",
    "    return response_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  multiple calls, vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:13:16.221778Z",
     "start_time": "2025-03-30T14:13:16.216054Z"
    }
   },
   "outputs": [],
   "source": [
    "def self_consistency_inference(\n",
    "    question_text: str,\n",
    "    model_name: str = \"deepseek-r1-distill-qwen-1.5b\",\n",
    "    num_samples: int = 5,\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9\n",
    ") -> tuple[str, str, float, dict]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        - best_answer (str): Most frequently voted final answer\n",
    "        - best_response (str): Full model response corresponding to best_answer\n",
    "        - confidence (float): vote_ratio = max_votes / num_samples\n",
    "        - usage (dict): prompt_tokens, completion_tokens, total_tokens from the selected response\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    sample_data = []\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        response_text, usage = call_model(\n",
    "            prompt=question_text,\n",
    "            model_name=model_name,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        final_ans = extract_final_answer(response_text)\n",
    "\n",
    "        sample_data.append({\n",
    "            \"answer\": final_ans,\n",
    "            \"response\": response_text,\n",
    "            \"usage\": usage  # full usage dict from API\n",
    "        })\n",
    "\n",
    "        print(f\"[Sample {i+1}] Raw Final Answer: {final_ans}\")\n",
    "\n",
    "    # Vote for the most common final answer\n",
    "    counter = Counter(s[\"answer\"] for s in sample_data)\n",
    "    best_answer, best_vote_count = counter.most_common(1)[0]\n",
    "    confidence = best_vote_count / num_samples\n",
    "\n",
    "    # Get the first response that produced the best_answer\n",
    "    for s in sample_data:\n",
    "        if s[\"answer\"] == best_answer:\n",
    "            best_response = s[\"response\"]\n",
    "            usage = s[\"usage\"]\n",
    "            break\n",
    "\n",
    "    return best_answer, best_response, confidence, usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Demo Question Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T14:13:50.949614Z",
     "start_time": "2025-03-30T14:13:18.373235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample 1] Raw Final Answer: 18\n",
      "[Sample 2] Raw Final Answer: 18\n",
      "[Sample 3] Raw Final Answer: 18\n",
      "[Sample 4] Raw Final Answer: 18\n",
      "[Sample 5] Raw Final Answer: 18\n",
      "=== Single Question Test ===\n",
      "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Predicted Final Answer: 18\n",
      "Gold Answer: 18\n",
      "Correct? True\n",
      "Response Length (chars): 1010\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# 5) Demo: Single Question Test\n",
    "###############################################################################\n",
    "if __name__ == \"__main__\":\n",
    "    # Example question\n",
    "    # question_text = \"If x = 3, what is the value of (x + 2)^2?\"\n",
    "    question_text = \"Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "\n",
    "    # Gold answer\n",
    "    gold_answer = \"18\"\n",
    "\n",
    "    # Do self-consistency\n",
    "    predicted, final_response, confidence, usage = self_consistency_inference(\n",
    "        question_text=question_text,\n",
    "        model_name=\"deepseek-r1-distill-qwen-1.5b\",  # or 'qwen2-math-1.5b-instruct' if valid\n",
    "        num_samples=5,          # how many times to sample\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "    # Compare with gold\n",
    "    is_correct = (predicted.strip() == gold_answer.strip())\n",
    "\n",
    "    response_length = len(final_response)\n",
    "\n",
    "    \n",
    "    print(\"=== Single Question Test ===\")\n",
    "    print(\"Question:\", question_text)\n",
    "    print(\"Predicted Final Answer:\", predicted)\n",
    "    print(\"Gold Answer:\", gold_answer)\n",
    "    print(\"Correct?\" , is_correct)\n",
    "    print(f\"Response Length (chars): {response_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSM8K main_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-03-30T14:13:50.956329Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== Evaluating Model: qwen2.5-math-1.5b-instruct ====================\n",
      "üîÅ Resuming from existing result file: results/results_gsm8k_qwen.csv\n",
      "‚è© Skipping already completed index 1318\n",
      "\n",
      "\n",
      "==================== Evaluating Model: deepseek-r1-distill-qwen-1.5b ====================\n",
      "\n",
      "=== Question 0 ===\n",
      "Question: Janet‚Äôs ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# =============================\n",
    "# Helper functions\n",
    "# =============================\n",
    "def extract_numeric(ans: str) -> str:\n",
    "    \"\"\"Clean extracted answer by removing $, units, commas, and extra tokens.\"\"\"\n",
    "    ans = ans.strip()\n",
    "    ans = re.sub(r\"[^0-9.\\-]\", \"\", ans)\n",
    "    return ans\n",
    "\n",
    "def extract_gold_answer(gold_text: str) -> str:\n",
    "    \"\"\"Extract the numeric answer from GSM8K-style '#### <number>' line.\"\"\"\n",
    "    match = re.search(r\"####\\s*(\\d+(?:\\.\\d+)?)\", str(gold_text))\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    numbers = re.findall(r\"\\d+(?:\\.\\d+)?\", str(gold_text))\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return str(gold_text).strip()\n",
    "\n",
    "def is_correct_with_tolerance(pred: str, gold: str, epsilon: float = 1e-5) -> bool:\n",
    "    \"\"\"Compare pred and gold within a tolerable margin of error.\"\"\"\n",
    "    try:\n",
    "        return abs(float(pred) - float(gold)) < epsilon\n",
    "    except ValueError:\n",
    "        return pred.strip() == gold.strip()\n",
    "    \n",
    "def is_abnormal_answer(ans: str, max_repeat: int = 10, max_len: int = 100) -> bool:\n",
    "    ans = ans.strip()\n",
    "\n",
    "    # Empty or overly long numeric string\n",
    "    if len(ans) > max_len:\n",
    "        return True\n",
    "\n",
    "    # Check for repeated characters like 777777...\n",
    "    if re.match(r\"^(\\d)\\1{\" + str(max_repeat) + r\",}$\", ans):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# =============================\n",
    "# Load GSM8K CSV Dataset\n",
    "# =============================\n",
    "df = pd.read_csv(\"dataset/GSM8K/main_test.csv\")\n",
    "num_samples = -1  # -1 for full dataset\n",
    "subset = df if num_samples == -1 else df.head(num_samples)\n",
    "\n",
    "# =============================\n",
    "# Models to Evaluate\n",
    "# =============================\n",
    "models_to_test = [\n",
    "    \"deepseek-r1-distill-qwen-1.5b\"\n",
    "]\n",
    "\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "samples_per_question = 5\n",
    "\n",
    "# =============================\n",
    "# Main Evaluation Loop\n",
    "# =============================\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n\\n==================== Evaluating Model: {model_name} ====================\")\n",
    "\n",
    "    model_short_names = {\n",
    "        \"deepseek-r1-distill-qwen-1.5b\": \"deepseek\"\n",
    "    }\n",
    "\n",
    "    dataset_name = \"gsm8k\"\n",
    "    short_name = model_short_names.get(model_name, model_name.replace(\"/\", \"_\"))\n",
    "    save_path = f\"results/zero_shot_{dataset_name}_{short_name}.csv\"\n",
    "\n",
    "    # Resume: Load existing result file\n",
    "    done_indices = set()\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(save_path)\n",
    "            done_indices = set(existing_df[\"index\"].tolist())\n",
    "            print(f\"üîÅ Resuming from existing result file: {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read existing result file: {e}\")\n",
    "\n",
    "    for idx, row in subset.iterrows():\n",
    "        if idx in done_indices:\n",
    "            if idx == len(done_indices)-1:\n",
    "                print(f\"‚è© Skipping already completed index {idx}\")\n",
    "            continue\n",
    "        # print(f\"‚è© Skipping already completed index {idx-1}\")\n",
    "        question = row[\"question\"]\n",
    "        gold_answer = row[\"answer\"]\n",
    "\n",
    "        print(f\"\\n=== Question {idx} ===\")\n",
    "        print(\"Question:\", question)\n",
    "\n",
    "        try:\n",
    "            pred, full_response, confidence, usage = self_consistency_inference(\n",
    "                question_text=question,\n",
    "                model_name=model_name,\n",
    "                num_samples=samples_per_question,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {e}\")\n",
    "            pred = \"\"\n",
    "            full_response = \"\"\n",
    "            confidence = 0.0\n",
    "            usage = {}\n",
    "\n",
    "        gold_clean = extract_gold_answer(gold_answer)\n",
    "\n",
    "        pred = extract_numeric(pred)\n",
    "        # Detect abnormal numeric patterns\n",
    "        if is_abnormal_answer(pred):\n",
    "            print(\"‚ö†Ô∏è Abnormal pattern detected! Marking as invalid.\")\n",
    "            pred = \"INVALID\" # Mark as invalid\n",
    "            correct = False\n",
    "\n",
    "        correct = is_correct_with_tolerance(pred, gold_clean)\n",
    "        response_length = len(full_response)\n",
    "\n",
    "        print(\"Gold Extracted:\", repr(gold_clean))\n",
    "        print(\"Predicted Final Answer:\", repr(pred))\n",
    "        print(\"Matched Correctly?\", correct)\n",
    "\n",
    "        # Extract token info from usage\n",
    "        completion_tokens = usage.get(\"output_tokens\", usage.get(\"completion_tokens\", 0))\n",
    "        prompt_tokens = usage.get(\"input_tokens\", usage.get(\"prompt_tokens\", 0))\n",
    "        total_tokens = usage.get(\"total_tokens\", 0)\n",
    "\n",
    "        result_row = {\n",
    "            \"index\": idx,\n",
    "            \"gold_clean\": gold_clean,\n",
    "            \"predicted_answer\": pred,\n",
    "            \"correct\": correct,\n",
    "            \"response_length\": response_length,\n",
    "            \"confidence\": confidence,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "        }\n",
    "\n",
    "        # Append to CSV immediately\n",
    "        write_header = not os.path.exists(save_path)\n",
    "        with open(save_path, mode='a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys())\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row)\n",
    "\n",
    "        print(f\"‚úÖ Saved result for index {idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AIME_1983_2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Load AIME CSV Dataset\n",
    "# =============================\n",
    "df = pd.read_csv(\"dataset/AIME_Dataset_1983_2024.csv\")  # üëà ËøôÈáåÊç¢Êàê‰Ω†ÁöÑ AIME Êï∞ÊçÆË∑ØÂæÑ\n",
    "num_samples = -1\n",
    "subset = df if num_samples == -1 else df.head(num_samples)\n",
    "\n",
    "# =============================\n",
    "# Models to Evaluate\n",
    "# =============================\n",
    "models_to_test = [\n",
    "    \"deepseek-r1-distill-qwen-1.5b\"\n",
    "]\n",
    "\n",
    "temperature = 0.7\n",
    "top_p = 0.9\n",
    "samples_per_question = 5\n",
    "\n",
    "# =============================\n",
    "# Main Evaluation Loop\n",
    "# =============================\n",
    "for model_name in models_to_test:\n",
    "    print(f\"\\n\\n==================== Evaluating Model: {model_name} ====================\")\n",
    "\n",
    "    model_short_names = {\n",
    "        \"deepseek-r1-distill-qwen-1.5b\": \"deepseek\"\n",
    "    }\n",
    "\n",
    "    dataset_name = \"aime\"\n",
    "    short_name = model_short_names.get(model_name, model_name.replace(\"/\", \"_\"))\n",
    "    save_path = f\"results/zero_shot_{dataset_name}_{short_name}.csv\"\n",
    "\n",
    "    # Resume\n",
    "    done_indices = set()\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(save_path)\n",
    "            done_indices = set(existing_df[\"index\"].tolist())\n",
    "            print(f\"üîÅ Resuming from existing result file: {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to read existing result file: {e}\")\n",
    "\n",
    "    for idx, row in subset.iterrows():\n",
    "        if idx in done_indices:\n",
    "            if idx == len(done_indices)-1:\n",
    "                print(f\"‚è© Skipping already completed index {idx}\")\n",
    "            continue\n",
    "\n",
    "        question = row[\"Question\"]\n",
    "        gold_answer = row[\"Answer\"]\n",
    "\n",
    "        print(f\"\\n=== Question {idx} ===\")\n",
    "        print(\"Question:\", question)\n",
    "\n",
    "        try:\n",
    "            pred, full_response, confidence, usage = self_consistency_inference(\n",
    "                question_text=question,\n",
    "                model_name=model_name,\n",
    "                num_samples=samples_per_question,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {e}\")\n",
    "            pred = \"\"\n",
    "            full_response = \"\"\n",
    "            confidence = 0.0\n",
    "            usage = {}\n",
    "\n",
    "        gold_clean = extract_gold_answer(gold_answer)\n",
    "        pred = extract_numeric(pred)\n",
    "\n",
    "        if is_abnormal_answer(pred):\n",
    "            print(\"‚ö†Ô∏è Abnormal pattern detected! Marking as invalid.\")\n",
    "            pred = \"INVALID\"\n",
    "            correct = False\n",
    "        else:\n",
    "            correct = is_correct_with_tolerance(pred, gold_clean)\n",
    "\n",
    "        response_length = len(full_response)\n",
    "\n",
    "        # Token usage\n",
    "        completion_tokens = usage.get(\"output_tokens\", usage.get(\"completion_tokens\", 0))\n",
    "        prompt_tokens = usage.get(\"input_tokens\", usage.get(\"prompt_tokens\", 0))\n",
    "        total_tokens = usage.get(\"total_tokens\", 0)\n",
    "\n",
    "        print(\"Gold Extracted:\", repr(gold_clean))\n",
    "        print(\"Predicted Final Answer:\", repr(pred))\n",
    "        print(\"Matched Correctly?\", correct)\n",
    "\n",
    "        result_row = {\n",
    "            \"index\": idx,\n",
    "            \"id\": row[\"ID\"],\n",
    "            \"year\": row[\"Year\"],\n",
    "            \"problem_number\": row[\"Problem Number\"],\n",
    "            \"gold_clean\": gold_clean,\n",
    "            \"predicted_answer\": pred,\n",
    "            \"correct\": correct,\n",
    "            \"response_length\": response_length,\n",
    "            \"confidence\": confidence,\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "        }\n",
    "\n",
    "        # Save row immediately\n",
    "        write_header = not os.path.exists(save_path)\n",
    "        with open(save_path, mode='a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys())\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row)\n",
    "\n",
    "        print(f\"‚úÖ Saved result for index {idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATH-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Index Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "###############################################################################\n",
    "# 1) Load dataset\n",
    "###############################################################################\n",
    "df = pd.read_csv(\"dataset/GSM8K/main_test.csv\")\n",
    "\n",
    "target_index = 154\n",
    "\n",
    "sample = df.iloc[target_index]\n",
    "question_text = sample[\"question\"]\n",
    "gold_answer_raw = sample[\"answer\"]\n",
    "\n",
    "###############################################################################\n",
    "# 2) Run self-consistency on this question\n",
    "###############################################################################\n",
    "predicted, final_response, confidence, usage = self_consistency_inference(\n",
    "    question_text=question_text,\n",
    "    model_name=\"deepseek-r1-distill-qwen-1.5b\",\n",
    "    num_samples=5,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 3) Evaluate\n",
    "###############################################################################\n",
    "gold_clean = extract_gold_answer(gold_answer_raw)\n",
    "pred_clean = extract_numeric(predicted)\n",
    "is_correct = (pred_clean == gold_clean)\n",
    "response_length = len(final_response)\n",
    "\n",
    "###############################################################################\n",
    "# 4) Print results\n",
    "###############################################################################\n",
    "print(\"\\n=== Single Question Test ===\")\n",
    "print(\"Index:\", target_index)\n",
    "print(\"Gold Raw:\", repr(gold_answer_raw))\n",
    "print(\"Question:\", question_text)\n",
    "print(\"Predicted Final Answer:\", repr(pred_clean))\n",
    "print(\"Gold Answer:\", repr(gold_clean))\n",
    "print(\"Correct?:\", is_correct)\n",
    "print(f\"Response Text: {final_response}\")\n",
    "print(f\"Response Length: {response_length} chars\")\n",
    "print(f\"Confidence: {confidence:.2f}\")\n",
    "print(\"Token Usage:\", usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sample 1] Raw Final Answer: 2051\n",
      "[Sample 2] Raw Final Answer: 2051\n",
      "[Sample 3] Raw Final Answer: 2051\n",
      "[Sample 4] Raw Final Answer: 2051\n",
      "[Sample 5] Raw Final Answer: 2051\n",
      "\n",
      "=== AIME Single Question Test ===\n",
      "Index: 154\n",
      "Year: 1995\n",
      "Problem Number: 3\n",
      "Question: Starting at $(0,0),$ an object moves in the coordinate plane via a sequence of steps, each of length one.  Each step is left, right, up, or down, all four equally likely.  Let $p$ be the probability that the object reaches $(2,2)$ in six or fewer steps.  Given that $p$ can be written in the form $m/n,$ where $m$ and $n$ are relatively prime positive integers, find $m+n.$\n",
      "Gold Raw: '67'\n",
      "Gold Cleaned: '67'\n",
      "Predicted Answer: '2051'\n",
      "Correct?: False\n",
      "Confidence: 1.00\n",
      "Response Length: 2211\n",
      "Token Usage: {\"input_tokens\": 153, \"output_tokens\": 672, \"total_tokens\": 825, \"cached_tokens\": 0}\n",
      "\n",
      "=== Full Model Response ===\n",
      "\n",
      "To determine the probability \\( p \\) that an object starting at \\((0,0)\\) reaches \\((2,2)\\) in six or fewer steps, we need to analyze the possible paths and their probabilities.\n",
      "\n",
      "First, note that to reach \\((2,2)\\) from \\((0,0)\\), the object must take exactly 2 steps right (R) and 2 steps up (U). This means the remaining 2 steps can be either left (L) or down (D), but they must cancel out the extra steps taken right or up. Therefore, the total number of steps must be exactly 4 (2 R and 2 U) because any additional steps would not help in reaching \\((2,2)\\) in fewer than 6 steps.\n",
      "\n",
      "The number of ways to arrange 2 R's, 2 U's, and 2 steps that cancel each other out (which can be either 2 L's and 2 D's, or 1 L, 1 D, and 2 steps that are not R or U) is the number of ways to arrange 4 R's and 2 U's in 6 steps, which is given by the binomial coefficient \\(\\binom{6}{2,2,2} = \\frac{6!}{2!2!2!} = 90\\).\n",
      "\n",
      "However, we need to ensure that the object does not move outside the bounds of the first quadrant (i.e., it does not move left or down more than necessary). The only valid paths are those that consist of exactly 2 R's, 2 U's, and 2 steps that are either L's or D's, but in such a way that the object does not move outside the first quadrant. The only valid path is the one where the extra steps are exactly 2 L's and 2 D's, which is not possible because it would take more than 6 steps to return to the first quadrant.\n",
      "\n",
      "Therefore, the only valid path is the one where the object takes exactly 2 R's and 2 U's in the first 4 steps, and then the remaining 2 steps are either L's or D's that cancel each other out. The number of valid paths is the number of ways to arrange 2 R's and 2 U's in 4 steps, which is \\(\\binom{4}{2} = 6\\).\n",
      "\n",
      "The total number of possible paths of length 6 is \\(4^6 = 4096\\), because each step has 4 possible directions.\n",
      "\n",
      "Thus, the probability \\( p \\) is given by the ratio of the number of valid paths to the total number of paths:\n",
      "\\[\n",
      "p = \\frac{6}{4096} = \\frac{3}{2048}\n",
      "\\]\n",
      "\n",
      "Since 3 and 2048 are relatively prime, the fraction is in its simplest form. Therefore, \\( m = 3 \\) and \\( n = 2048 \\), and the final answer is:\n",
      "\\[\n",
      "m + n = 3 + 2048 = 2051\n",
      "\\]\n",
      "\n",
      "The answer is:\n",
      "\\[\n",
      "\\boxed{2051}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "###############################################################################\n",
    "# 1) Load AIME Dataset\n",
    "###############################################################################\n",
    "df = pd.read_csv(\"dataset/AIME_Dataset_1983_2024.csv\") \n",
    "\n",
    "target_index = 154  # üëà Ë¶ÅÊµãËØïÂì™‰∏ÄÈÅìÈ¢òÔºåÊîπËøôÈáå\n",
    "\n",
    "sample = df.iloc[target_index]\n",
    "question_text = sample[\"Question\"]\n",
    "gold_answer_raw = sample[\"Answer\"]\n",
    "\n",
    "###############################################################################\n",
    "# 2) Run self-consistency inference on this question\n",
    "###############################################################################\n",
    "predicted, full_response, confidence, usage = self_consistency_inference(\n",
    "    question_text=question_text,\n",
    "    model_name=\"deepseek-r1-distill-qwen-1.5b\", \n",
    "    num_samples=5,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "###############################################################################\n",
    "# 3) Evaluate Answer\n",
    "###############################################################################\n",
    "gold_clean = extract_gold_answer(gold_answer_raw)\n",
    "pred_clean = extract_numeric(predicted)\n",
    "is_correct = is_correct_with_tolerance(pred_clean, gold_clean)\n",
    "response_length = len(full_response)\n",
    "\n",
    "###############################################################################\n",
    "# 4) Print Results\n",
    "###############################################################################\n",
    "print(\"\\n=== AIME Single Question Test ===\")\n",
    "print(\"Index:\", target_index)\n",
    "print(\"Year:\", sample.get(\"Year\", \"N/A\"))\n",
    "print(\"Problem Number:\", sample.get(\"Problem Number\", \"N/A\"))\n",
    "print(\"Question:\", question_text)\n",
    "print(\"Gold Raw:\", repr(gold_answer_raw))\n",
    "print(\"Gold Cleaned:\", repr(gold_clean))\n",
    "print(\"Predicted Answer:\", repr(pred_clean))\n",
    "print(\"Correct?:\", is_correct)\n",
    "print(\"Confidence:\", f\"{confidence:.2f}\")\n",
    "print(\"Response Length:\", response_length)\n",
    "print(\"Token Usage:\", usage)\n",
    "print(\"\\n=== Full Model Response ===\\n\")\n",
    "print(full_response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
