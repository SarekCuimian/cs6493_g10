import os
import csv
import time
import pandas as pd
import dashscope
import re
from dotenv import load_dotenv

# ========== åŠ è½½APIå¯†é’¥ ==========
load_dotenv("dashscope_api_key.env")
api_key = os.getenv("DASHSCOPE_API_KEY")
if not api_key:
    print("âŒ DASHSCOPE_API_KEY not found!")


def clean_latex_expression(expr: str) -> str:
    """è§„èŒƒåŒ–LaTeXè¡¨è¾¾å¼åˆ°å¯æ¯”è¾ƒçš„æ ¼å¼"""
    # ç¬¬ä¸€æ­¥ï¼šç§»é™¤æ‰€æœ‰æ’ç‰ˆå‘½ä»¤å’Œå¤šä½™å­—ç¬¦ï¼ˆä¿®å¤æ­£åˆ™è¡¨è¾¾å¼ï¼‰
    expr = re.sub(
        r'\$?(left|right|big[gl]?|Big[gl]?|\,|:|;|mathrm|text|mathbf|boxed|displaystyle)'
        r'|[{}()\$]|\s+',
        '',
        expr
    )

    # ç»Ÿä¸€æ•°å­¦ç¬¦å·è¡¨ç¤º
    replacements = [
        (r'\\pi|Ï€', 'Ï€'),  # ç»Ÿä¸€å¸Œè…Šå­—æ¯
        (r'\\times|\*|â‹…', 'Ã—'),  # ç»Ÿä¸€ä¹˜æ³•ç¬¦å·
        (r'\\frac{([^}]+)}{([^}]+)}', r'\1/\2'),  # åˆ†æ•°è½¬æ¢
        (r'^\\', ''),  # ç§»é™¤æ®‹ç•™å‘½ä»¤ç¬¦
    ]

    for pattern, repl in replacements:
        expr = re.sub(pattern, repl, expr)

    # æœ€ç»ˆæ¸…ç†ï¼ˆè½¬ä¸ºå°å†™å»é™¤éåŒ¹é…å·®å¼‚ï¼‰
    return expr.lower().strip()


# ========== å¢å¼ºå‹ç­”æ¡ˆæå– ==========
def extract_final_answer(text: str) -> str:
    """ä»æ¨¡å‹å“åº”ä¸­æå–å¹¶è§„èŒƒåŒ–æœ€ç»ˆç­”æ¡ˆ"""
    try:
        # ä¼˜å…ˆçº§1ï¼šæå–boxedå†…å®¹
        boxed_match = re.search(r'\\boxed{([^{}]*)}', text, re.DOTALL)
        if boxed_match:
            return clean_latex_expression(boxed_match.group(1))

        # ä¼˜å…ˆçº§2ï¼šFinal Answeræ¨¡å¼
        final_match = re.search(
            r'Final Answer\s*:\s*([^\n$]+)(?:\$|\\boxed|$)',
            text,
            re.IGNORECASE | re.DOTALL
        )
        if final_match:
            return clean_latex_expression(final_match.group(1))

        # ä¼˜å…ˆçº§3ï¼šæŸ¥æ‰¾æœ€åä¸€ä¸ªæ•°å­¦è¡¨è¾¾å¼
        math_matches = re.findall(r'\$([^$]+)\$', text)
        if math_matches:
            return clean_latex_expression(math_matches[-1])

        # å›é€€ç­–ç•¥ï¼šæœ€åéç©ºè¡Œ
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return clean_latex_expression(lines[-1]) if lines else ""

    except Exception as e:
        print(f"âš ï¸ Error extracting answer: {str(e)}")
        return ""


# ========== å¢å¼ºå‹åˆ¤æ–­é€»è¾‘ ==========
def judge_answers(
        responses_texts: list[str],
        reference_answer: str,
        judge_model: str,
        system_prompt: str = "æ¯”è¾ƒå›ç­”ä¸å‚è€ƒç­”æ¡ˆçš„æ•°å­¦ç­‰ä»·æ€§ã€‚æœ€åç”¨'Correct: True/False'å’Œ'SelectedIndices: [æ•°å­—åˆ—è¡¨]'åˆ†è¡Œè¾“å‡º",
        temperature: float = 0.0,
        top_p: float = 1.0,
        max_retries: int = 3,
        retry_delay: float = 0.3
) -> dict:
    # æ ‡å‡†åŒ–å‚è€ƒç­”æ¡ˆ
    ref_clean = clean_latex_expression(reference_answer)
    print(f"\nğŸ” å‚è€ƒç­”æ¡ˆæ ‡å‡†åŒ–ç»“æœ: {ref_clean}")

    # æ”¶é›†æ‰€æœ‰æ ‡å‡†åŒ–åçš„å›ç­”
    final_answers = [extract_final_answer(text) for text in responses_texts]
    print("ğŸ“ æ¨¡å‹å›ç­”æ ‡å‡†åŒ–ç»“æœ:")
    for i, ans in enumerate(final_answers, 1):
        print(f"  å›ç­”{i}: {ans}")

    # é˜¶æ®µ1ï¼šç²¾ç¡®åŒ¹é…æ£€æŸ¥
    exact_matches = [i + 1 for i, ans in enumerate(final_answers) if ans == ref_clean]
    if exact_matches:
        print(f"âœ… ç²¾ç¡®åŒ¹é…åˆ°å›ç­”: {exact_matches}")
        return {
            "correct": True,
            "selected_indices": exact_matches,
            "judge_response": f"Exact matches found: {exact_matches}"
        }

    # é˜¶æ®µ2ï¼šæ•°å€¼ç­‰ä»·æ£€æŸ¥
    try:
        ref_num = float(ref_clean)
        num_matches = []
        for i, ans in enumerate(final_answers):
            try:
                if abs(float(ans) - ref_num) < 1e-6:
                    num_matches.append(i + 1)
            except ValueError:
                continue
        if num_matches:
            print(f"ğŸ”¢ æ•°å€¼ç­‰ä»·åŒ¹é…: {num_matches}")
            return {
                "correct": True,
                "selected_indices": num_matches,
                "judge_response": f"Numerical matches: {num_matches}"
            }
    except ValueError:
        pass

    # é˜¶æ®µ3ï¼šè°ƒç”¨å¤§æ¨¡å‹åˆ¤æ–­
    user_prompt = f"""\nå‚è€ƒç­”æ¡ˆï¼š{ref_clean}\n\nè¯·åˆ¤æ–­ä»¥ä¸‹å›ç­”æ˜¯å¦ä¸å‚è€ƒç­”æ¡ˆæ•°å­¦ç­‰ä»·ï¼š
    """ + '\n'.join([f"{i + 1}. {ans}" for i, ans in enumerate(final_answers)])

    for attempt in range(max_retries):
        try:
            response = dashscope.Generation.call(
                api_key=api_key,
                model=judge_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=temperature,
                top_p=top_p,
                result_format="message"
            )

            if response.status_code != 200:
                raise Exception(f"APIå“åº”å¼‚å¸¸: {response}")

            judge_text = response.output.choices[0].message.content
            print(f"\nğŸ§  å¤§æ¨¡å‹åˆ¤æ–­åŸå§‹å“åº”:\n{judge_text}")

            # è§£æåˆ¤æ–­ç»“æœ
            correct = "correct: true" in judge_text.lower()
            indices = [int(i) for i in re.findall(r'\b\d+\b', judge_text) if i.isdigit()]
            valid_indices = [i for i in indices if 1 <= i <= len(responses_texts)]

            return {
                "correct": correct or bool(valid_indices),
                "selected_indices": valid_indices or [],
                "judge_response": judge_text
            }

        except Exception as e:
            print(f"âš ï¸ ç¬¬{attempt + 1}æ¬¡åˆ¤æ–­è¯·æ±‚å¤±è´¥: {str(e)}")
            time.sleep(retry_delay)

    # é˜¶æ®µ4ï¼šå¤šæ•°æŠ•ç¥¨å›é€€
    answer_counts = {}
    for ans in final_answers:
        answer_counts[ans] = answer_counts.get(ans, 0) + 1
    majority_answer, max_count = max(answer_counts.items(), key=lambda x: x[1])

    selected_indices = [i + 1 for i, ans in enumerate(final_answers) if ans == majority_answer]
    correct_status = (majority_answer == ref_clean) or (max_count / len(final_answers) > 0.5)

    print(f"âš–ï¸ å¤šæ•°æŠ•ç¥¨ç»“æœï¼š{selected_indices} (ç½®ä¿¡åº¦ {max_count / len(final_answers):.0%})")
    return {
        "correct": correct_status,
        "selected_indices": selected_indices,
        "judge_response": "Fallback to majority voting"
    }


# ========== æ¨¡å‹è°ƒç”¨æ¨¡å— ==========
def call_model(
        prompt: str,
        model_name: str,
        system_prompt: str = (
                "ä½ æ˜¯ä¸€ä½æ•°å­¦é—®é¢˜è§£å†³ä¸“å®¶ã€‚è¯·ä½¿ç”¨é€æ­¥æ¨ç†ï¼ˆæ€ç»´é“¾ï¼‰å±•ç¤ºæ‚¨çš„è§£é¢˜è¿‡ç¨‹ï¼Œ"
                "å¹¶åœ¨æœ€åä»¥æ ¼å¼'Final Answer: <ç­”æ¡ˆ>'ç»™å‡ºæœ€ç»ˆç»“è®ºã€‚"
        ),
        temperature: float = 0.7,
        top_p: float = 0.9,
        max_retries: int = 3,
        retry_delay: float = 0.3
) -> tuple[str, dict]:
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": prompt}
    ]

    for attempt in range(max_retries):
        try:
            response = dashscope.Generation.call(
                api_key=api_key,
                model=model_name,
                messages=messages,
                temperature=temperature,
                top_p=top_p,
                result_format="message"
            )

            if response.get("status_code") == 429:
                print(f"â³ ç¬¬{attempt + 1}æ¬¡é‡è¯•ï¼ˆé¢‘ç‡é™åˆ¶ï¼‰...")
                time.sleep(retry_delay)
                continue

            if response.status_code != 200:
                raise Exception(f"APIé”™è¯¯ä»£ç  {response.status_code}")

            content = response.output.choices[0].message.content
            usage = response.usage or {}
            return content, usage

        except Exception as e:
            print(f"âš ï¸ ç¬¬{attempt + 1}æ¬¡è°ƒç”¨å¤±è´¥: {str(e)}")
            time.sleep(retry_delay)

    print("âŒ æ¨¡å‹è°ƒç”¨å…¨éƒ¨å¤±è´¥")
    return "", {}


# ========== ä¸»æ‰§è¡Œæµç¨‹ ==========
if __name__ == "__main__":
    # é…ç½®å‚æ•°
    data_path = "dataset/MATH-500/math500_processed.csv"
    models_to_test = ["qwen2.5-math-1.5b-instruct"]
    samples_per_question = 3
    temperature = 0.7
    top_p = 0.9

    # åŠ è½½æ•°æ®é›†
    df = pd.read_csv(data_path)
    print(f"ğŸ“‚ å·²åŠ è½½æ•°æ®é›†ï¼Œå…±{len(df)}æ¡é—®é¢˜")

    for model_name in models_to_test:
        print(f"\n{'=' * 30} å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name} {'=' * 30}")

        # åˆå§‹åŒ–ç»“æœæ–‡ä»¶
        save_path = f"results/COT_math500_{model_name.split('/')[-1]}.csv"
        done_indices = set()

        if os.path.exists(save_path):
            try:
                existing_df = pd.read_csv(save_path)
                done_indices = set(existing_df["index"].tolist())
                print(f"â© ä»{save_path}æ¢å¤è¿›åº¦ï¼Œå·²å¤„ç†{len(done_indices)}ä¸ªé—®é¢˜")
            except Exception as e:
                print(f"âš ï¸ æ¢å¤è¿›åº¦å¤±è´¥: {str(e)}")

        # éå†æ•°æ®é›†
        for idx, row in df.iterrows():
            if idx in done_indices:
                continue

            problem = row["problem"]
            answer = row["answer"]

            print(f"\n{'=' * 20} é—®é¢˜ #{idx} {'=' * 20}")
            print(f"â“ é—®é¢˜å†…å®¹:\n{problem}\n")
            print(f"ğŸ”‘ å‚è€ƒç­”æ¡ˆ: {answer}")

            # æ”¶é›†æ¨¡å‹å›ç­”
            responses = []
            usages = []
            total_completion_tokens = 0
            total_prompt_tokens = 0
            total_total_tokens = 0
            for i in range(samples_per_question):
                print(f"\nğŸ”„ ç”Ÿæˆå›ç­” {i + 1}/{samples_per_question}...")
                response, usage = call_model(problem, model_name)
                responses.append(response)
                usages.append(usage)
                total_completion_tokens += usage.get('completion_tokens', 0)
                total_prompt_tokens += usage.get('prompt_tokens', 0)
                total_total_tokens += usage.get('total_tokens', 0)
                print(f"ç”Ÿæˆçš„å›ç­”:\n{'-' * 20}\n{response}\n{'-' * 20}")

            # è®¡ç®—æ€»å“åº”é•¿åº¦
            response_length = sum(len(response) for response in responses)

            # åˆ¤æ–­ç­”æ¡ˆæ­£ç¡®æ€§
            print("\nğŸ” å¼€å§‹åˆ¤æ–­å›ç­”æ­£ç¡®æ€§...")
            judgement = judge_answers(
                responses_texts=responses,
                reference_answer=answer,
                judge_model=model_name
            )

            # è®°å½•ç»“æœ
            result_row = {
                "index": idx,
                "gold_answer": answer,
                "correct": judgement["correct"],
                "response_length": response_length,
                "confidence": len(judgement["selected_indices"]) / samples_per_question,
                "completion_tokens": total_completion_tokens,
                "prompt_tokens": total_prompt_tokens,
                "total_tokens": total_total_tokens
            }

            # ä¿å­˜åˆ°CSV
            write_header = not os.path.exists(save_path)
            with open(save_path, "a", newline="", encoding="utf-8") as f:
                writer = csv.DictWriter(f, fieldnames=result_row.keys())
                if write_header:
                    writer.writeheader()
                writer.writerow(result_row)

            print(f"\nâœ”ï¸ ä¿å­˜ç»“æœ: {'æ­£ç¡®' if judgement['correct'] else 'é”™è¯¯'} " +
                  f"(ç½®ä¿¡åº¦ {result_row['confidence']:.0%})")
