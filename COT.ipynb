{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "from collections import Counter\n",
    "import dashscope\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# 加载环境变量中的 API 密钥\n",
    "load_dotenv(\"dashscope_api_key.env\")\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"❌ DASHSCOPE_API_KEY not found!\")\n",
    "\n",
    "\n",
    "def call_model(\n",
    "    prompt: str,\n",
    "    model_name: str = \"qwen2.5-math-1.5b-instruct\",\n",
    "    system_prompt: str = (\n",
    "        \"You are a helpful assistant. \"\n",
    "        \"Please show your reasoning step by step (Chain of Thought). \"\n",
    "        \"Then, on a new line at the end, write:\\n\"\n",
    "        \"'Final Answer: <the numeric result>'.\"\n",
    "    ),\n",
    "    temperature: float = 0.7,\n",
    "    top_p: float = 0.9,\n",
    "    max_retries: int = 3,\n",
    "    retry_delay: float = 0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    调用 DashScope 模型，带有简单的错误处理和明确的提示，要求模型输出最终答案行。\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        response = dashscope.Generation.call(\n",
    "            api_key=api_key,\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            result_format=\"message\"\n",
    "        )\n",
    "\n",
    "        status_code = response.get(\"status_code\", None)\n",
    "        if status_code == 429:\n",
    "            print(f\"⏳ [Attempt {attempt + 1}/{max_retries}] Rate limit! Wait {retry_delay}s...\")\n",
    "            time.sleep(retry_delay)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 从响应中提取内容和令牌使用情况\n",
    "            content = response[\"output\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            usage = response.get(\"usage\", {})\n",
    "            return content, usage\n",
    "        except (TypeError, KeyError, IndexError):\n",
    "            print(f\"⚠️ [Attempt {attempt + 1}/{max_retries}] Unexpected structure:\\n{response}\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "    print(f\"❌ All {max_retries} attempts failed, returning empty.\")\n",
    "    return \"\", {}\n",
    "\n",
    "\n",
    "def extract_final_answer(response_text: str):\n",
    "    \"\"\"\n",
    "    从模型响应中提取最终的数字答案。\n",
    "    \"\"\"\n",
    "    # 尝试匹配 \\boxed{...} 格式\n",
    "    box_match = re.search(r\"\\\\boxed\\{([^}]*)\\}\", response_text, flags=re.DOTALL)\n",
    "    if box_match:\n",
    "        box_content = box_match.group(1)\n",
    "        numbers_in_box = re.findall(r\"\\d+(?:\\.\\d+)?\", box_content)\n",
    "        if numbers_in_box:\n",
    "            return numbers_in_box[0].strip()\n",
    "        return box_content.strip()\n",
    "\n",
    "    # 尝试匹配 \"Final Answer: ...\" 格式\n",
    "    fa_match = re.search(r\"(?i)final answer\\s*:\\s*([^\\n]*)\", response_text)\n",
    "    if fa_match:\n",
    "        return fa_match.group(1).strip()\n",
    "\n",
    "    # 回退到匹配文本中的最后一个数字\n",
    "    numbers = re.findall(r\"\\d+(?:\\.\\d+)?\", response_text)\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "\n",
    "    # 最终回退：返回整个字符串\n",
    "    return response_text.strip()\n",
    "\n",
    "\n",
    "def extract_numeric(ans: str):\n",
    "    \"\"\"\n",
    "    清理提取的答案，去除 $、单位、逗号和额外的标记。\n",
    "    \"\"\"\n",
    "    ans = ans.strip()\n",
    "    ans = re.sub(r\"[^0-9.\\-]\", \"\", ans)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def extract_gold_answer(gold_text: str):\n",
    "    \"\"\"\n",
    "    从 GSM8K 风格的 '#### <number>' 行中提取数字答案。\n",
    "    \"\"\"\n",
    "    match = re.search(r\"####\\s*(\\d+(?:\\.\\d+)?)\", str(gold_text))\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    numbers = re.findall(r\"\\d+(?:\\.\\d+)?\", str(gold_text))\n",
    "    if numbers:\n",
    "        return numbers[-1]\n",
    "    return str(gold_text).strip()\n",
    "\n",
    "\n",
    "def is_correct_with_tolerance(pred: str, gold: str, epsilon: float = 1e-5):\n",
    "    \"\"\"\n",
    "    在可容忍的误差范围内比较预测答案和真实答案。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return abs(float(pred) - float(gold)) < epsilon\n",
    "    except ValueError:\n",
    "        return pred.strip() == gold.strip()\n",
    "\n",
    "\n",
    "def is_abnormal_answer(ans: str, max_repeat: int = 10, max_len: int = 100):\n",
    "    ans = ans.strip()\n",
    "\n",
    "    # 空字符串或过长的数字字符串\n",
    "    if len(ans) > max_len:\n",
    "        return True\n",
    "\n",
    "    # 检查是否有重复字符，如 777777...\n",
    "    if re.match(r\"^(\\d)\\1{\" + str(max_repeat) + r\",}$\", ans):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def evaluate_model_on_dataset(df, model_name, dataset_name, model_short_names):\n",
    "    short_name = model_short_names.get(model_name, model_name.replace(\"/\", \"_\"))\n",
    "    save_path = f\"results/self_consistency_{dataset_name}_{short_name}.csv\"\n",
    "\n",
    "    # 恢复：加载现有的结果文件\n",
    "    done_indices = set()\n",
    "    if os.path.exists(save_path):\n",
    "        try:\n",
    "            existing_df = pd.read_csv(save_path)\n",
    "            done_indices = set(existing_df[\"index\"].tolist())\n",
    "            print(f\"🔁 Resuming from existing result file: {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to read existing result file: {e}\")\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx in done_indices:\n",
    "            if idx == len(done_indices) - 1:\n",
    "                print(f\"⏩ Skipping already completed index {idx}\")\n",
    "            continue\n",
    "\n",
    "        if dataset_name == \"gsm8k\":\n",
    "            question = row[\"question\"]\n",
    "            gold_answer = row[\"answer\"]\n",
    "        elif dataset_name == \"aime\":\n",
    "            question = row[\"Question\"]\n",
    "            gold_answer = row[\"Answer\"]\n",
    "\n",
    "        print(f\"\\n=== Question {idx} ===\")\n",
    "        print(\"Question:\", question)\n",
    "\n",
    "        try:\n",
    "            response_text, usage = call_model(\n",
    "                prompt=question,\n",
    "                model_name=model_name,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9\n",
    "            )\n",
    "            pred = extract_final_answer(response_text)\n",
    "        except Exception as e:\n",
    "            print(f\"[Error] {e}\")\n",
    "            pred = \"\"\n",
    "            response_text = \"\"\n",
    "            usage = {}\n",
    "\n",
    "        gold_clean = extract_gold_answer(gold_answer)\n",
    "        pred = extract_numeric(pred)\n",
    "\n",
    "        if is_abnormal_answer(pred):\n",
    "            print(\"⚠️ Abnormal pattern detected! Marking as invalid.\")\n",
    "            pred = \"INVALID\"\n",
    "            correct = False\n",
    "        else:\n",
    "            correct = is_correct_with_tolerance(pred, gold_clean)\n",
    "\n",
    "        response_length = len(response_text)\n",
    "\n",
    "        print(\"Gold Extracted:\", repr(gold_clean))\n",
    "        print(\"Predicted Final Answer:\", repr(pred))\n",
    "        print(\"Matched Correctly?\", correct)\n",
    "\n",
    "        completion_tokens = usage.get(\"output_tokens\", usage.get(\"completion_tokens\", 0))\n",
    "        prompt_tokens = usage.get(\"input_tokens\", usage.get(\"prompt_tokens\", 0))\n",
    "        total_tokens = usage.get(\"total_tokens\", 0)\n",
    "\n",
    "        if dataset_name == \"gsm8k\":\n",
    "            result_row = {\n",
    "                \"index\": idx,\n",
    "                \"gold_clean\": gold_clean,\n",
    "                \"predicted_answer\": pred,\n",
    "                \"correct\": correct,\n",
    "                \"response_length\": response_length,\n",
    "                \"confidence\": 0,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "            }\n",
    "        elif dataset_name == \"aime\":\n",
    "            result_row = {\n",
    "                \"index\": idx,\n",
    "                \"id\": row[\"ID\"],\n",
    "                \"year\": row[\"Year\"],\n",
    "                \"problem_number\": row[\"Problem Number\"],\n",
    "                \"gold_clean\": gold_clean,\n",
    "                \"predicted_answer\": pred,\n",
    "                \"correct\": correct,\n",
    "                \"response_length\": response_length,\n",
    "                \"confidence\": 0,\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"total_tokens\": total_tokens,\n",
    "            }\n",
    "\n",
    "        # 立即追加到 CSV 文件\n",
    "        write_header = not os.path.exists(save_path)\n",
    "        with open(save_path, mode='a', newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=result_row.keys())\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(result_row)\n",
    "\n",
    "        print(f\"✅ Saved result for index {idx}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 加载 GSM8K 数据集\n",
    "    df_gsm8k = pd.read_csv(\"dataset/GSM8K/main_test.csv\")\n",
    "    num_samples_gsm8k = -1  # -1 表示使用完整数据集\n",
    "    subset_gsm8k = df_gsm8k if num_samples_gsm8k == -1 else df_gsm8k.head(num_samples_gsm8k)\n",
    "\n",
    "    # 加载 AIME 数据集\n",
    "    df_aime = pd.read_csv(\"dataset/AIME_Dataset_1983_2024.csv\")\n",
    "    num_samples_aime = -1\n",
    "    subset_aime = df_aime if num_samples_aime == -1 else df_aime.head(num_samples_aime)\n",
    "\n",
    "    # 要评估的模型\n",
    "    models_to_test = [\n",
    "        \"qwen2.5-math-1.5b-instruct\",\n",
    "        \"deepseek-r1-distill-qwen-1.5b\"\n",
    "    ]\n",
    "\n",
    "    model_short_names = {\n",
    "        \"qwen2.5-math-1.5b-instruct\": \"qwen\",\n",
    "    }\n",
    "\n",
    "    # 在 GSM8K 数据集上评估模型\n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\n\\n==================== Evaluating Model: {model_name} on GSM8K ====================\")\n",
    "        evaluate_model_on_dataset(subset_gsm8k, model_name, \"gsm8k\", model_short_names)\n",
    "\n",
    "    # 在 AIME 数据集上评估模型\n",
    "    for model_name in models_to_test:\n",
    "        print(f\"\\n\\n==================== Evaluating Model: {model_name} on AIME ====================\")\n",
    "        evaluate_model_on_dataset(subset_aime, model_name, \"aime\", model_short_names)\n",
    "\n",
    "    # GSM8K 单题测试\n",
    "    target_index_gsm8k = 154\n",
    "    sample_gsm8k = df_gsm8k.iloc[target_index_gsm8k]\n",
    "    question_text_gsm8k = sample_gsm8k[\"question\"]\n",
    "    gold_answer_raw_gsm8k = sample_gsm8k[\"answer\"]\n",
    "\n",
    "    response_text_gsm8k, usage_gsm8k = call_model(\n",
    "        prompt=question_text_gsm8k,\n",
    "        model_name=\"deepseek-r1-distill-qwen-1.5b\",\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    predicted_gsm8k = extract_final_answer(response_text_gsm8k)\n",
    "\n",
    "    gold_clean_gsm8k = extract_gold_answer(gold_answer_raw_gsm8k)\n",
    "    pred_clean_gsm8k = extract_numeric(predicted_gsm8k)\n",
    "    is_correct_gsm8k = (pred_clean_gsm8k == gold_clean_gsm8k)\n",
    "    response_length_gsm8k = len(response_text_gsm8k)\n",
    "\n",
    "    print(\"\\n=== GSM8K Single Question Test ===\")\n",
    "    print(\"Index:\", target_index_gsm8k)\n",
    "    print(\"Gold Raw:\", repr(gold_answer_raw_gsm8k))\n",
    "    print(\"Question:\", question_text_gsm8k)\n",
    "    print(\"Predicted Final Answer:\", repr(pred_clean_gsm8k))\n",
    "    print(\"Gold Answer:\", repr(gold_clean_gsm8k))\n",
    "    print(\"Correct?:\", is_correct_gsm8k)\n",
    "    print(f\"Response Text: {response_text_gsm8k}\")\n",
    "    print(f\"Response Length: {response_length_gsm8k} chars\")\n",
    "    print(f\"Confidence: 0\")\n",
    "    print(\"Token Usage:\", usage_gsm8k)\n",
    "\n",
    "    # AIME 单题测试\n",
    "    target_index_aime = 154\n",
    "    sample_aime = df_aime.iloc[target_index_aime]\n",
    "    question_text_aime = sample_aime[\"Question\"]\n",
    "    gold_answer_raw_aime = sample_aime[\"Answer\"]\n",
    "\n",
    "    response_text_aime, usage_aime = call_model(\n",
    "        prompt=question_text_aime,\n",
    "        model_name=\"qwen2.5-math-1.5b-instruct\",\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    predicted_aime = extract_final_answer(response_text_aime)\n",
    "\n",
    "    gold_clean_aime = extract_gold_answer(gold_answer_raw_aime)\n",
    "    pred_clean_aime = extract_numeric(predicted_aime)\n",
    "    is_correct_aime = is_correct_with_tolerance(pred_clean_aime, gold_clean_aime)\n",
    "    response_length_aime = len(response_text_aime)\n",
    "\n",
    "    print(\"\\n=== AIME Single Question Test ===\")\n",
    "    print(\"Index:\", target_index_aime)\n",
    "    print(\"Year:\", sample_aime.get(\"Year\", \"N/A\"))\n",
    "    print(\"Problem Number:\", sample_aime.get(\"Problem Number\", \"N/A\"))\n",
    "    print(\"Question:\", question_text_aime)\n",
    "    print(\"Gold Raw:\", repr(gold_answer_raw_aime))\n",
    "    print(\"Gold Cleaned:\", repr(gold_clean_aime))\n",
    "    print(\"Predicted Answer:\", repr(pred_clean_aime))\n",
    "    print(\"Correct?:\", is_correct_aime)\n",
    "    print(\"Confidence:\", f\"{0:.2f}\")\n",
    "    print(\"Response Length:\", response_length_aime)\n",
    "    print(\"Token Usage:\", usage_aime)\n",
    "    print(\"\\n=== Full Model Response ===\\n\")\n",
    "    print(response_text_aime)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
